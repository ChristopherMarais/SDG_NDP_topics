{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6e331b6-e00f-4592-b44b-ef5031439bad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-15T00:02:07.443266Z",
     "iopub.status.busy": "2022-10-15T00:02:07.443266Z",
     "iopub.status.idle": "2022-10-15T00:02:14.566398Z",
     "shell.execute_reply": "2022-10-15T00:02:14.565397Z",
     "shell.execute_reply.started": "2022-10-15T00:02:07.443266Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81c62921-cb96-457d-9d9e-6359f5761be5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-15T00:02:14.568378Z",
     "iopub.status.busy": "2022-10-15T00:02:14.567390Z",
     "iopub.status.idle": "2022-10-15T00:02:14.943378Z",
     "shell.execute_reply": "2022-10-15T00:02:14.943378Z",
     "shell.execute_reply.started": "2022-10-15T00:02:14.568378Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import data\n",
    "# data_df = pd.read_csv('C:\\\\Users\\\\gcmar\\\\Desktop\\\\GIT_REPOS\\\\LAB\\\\Literature_summary\\\\Papers\\\\scopus_bark_ambrosia_beetles.csv')\n",
    "# data_df['text'] = data_df['Title'].apply(str) + data_df['Author Keywords'].apply(str) + data_df['Index Keywords'].apply(str) + data_df['Abstract'].apply(str) \n",
    "# data_df['topic_text'] = data_df['Title'].apply(str) + data_df['Author Keywords'].apply(str) + data_df['Index Keywords'].apply(str)\n",
    "# docs = data_df['text'].dropna().tolist()\n",
    "# topic_docs = data_df['topic_text'].dropna().tolist()\n",
    "\n",
    "# # file_lst = data_df['file'].unique()\n",
    "# for j in file_lst:\n",
    "#     try:\n",
    "#         print('_______________________________\\n\\n', j)\n",
    "#         df = data_df[data_df['file']==j]\n",
    "#         for i in range(start_point, len(df), win_size):\n",
    "#             coref_series = coref_res(text_series=df[\"text\"].iloc[i:i+win_size])\n",
    "#             print('coref done', i, '-', i+win_size)\n",
    "#             link_entities_series = link_entities(text_series=coref_series)\n",
    "#             print('entity linking done', i, '-', i+win_size)\n",
    "#             entities_df = pd.concat(link_entities_series.tolist())\n",
    "#             print('df create done', i, '-', i+win_size)\n",
    "#             entities_df_lst.append(entities_df)\n",
    "#             print('df to list done', i, '-', i+win_size, '\\n')\n",
    "#         all_entities_df = pd.concat(entities_df_lst)\n",
    "#         all_entities_df.reset_index(drop=True, inplace=True)\n",
    "#         edge_lst_df = all_entities_df.value_counts().reset_index().rename(columns={0: \"count\"})\n",
    "#         edge_lst_df.to_csv('entity_weighted_edgelist_ALL_'+j[:-4]+'.csv')\n",
    "#         edge_lst_df.to_feather('entity_weighted_edgelist_ALL_'+j[:-4]+'.feather')\n",
    "#     except:\n",
    "#         print(\"DID NOT WORK WITH - \", j)\n",
    "\n",
    "pwd = os.getcwd()\n",
    "data_path = os.path.dirname(pwd) + \"\\\\Data\\\\NDPs\\\\\"\n",
    "data_df = pd.read_feather(data_path+\"docs.feather\")\n",
    "data_df = data_df.drop_duplicates().reset_index(drop=True)\n",
    "data_df[\"text\"] = data_df[\"text\"].astype(str)\n",
    "data_df['text'] = data_df['text'].str.strip()\n",
    "data_df['text'] = data_df['text'].str.split('. \\n') # OR '. \\n'\n",
    "data_df = data_df.explode('text').reset_index(drop=True)\n",
    "# docs = data_df['text'].dropna().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "580982bc-b186-4370-bb2d-ac16d1db2421",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-15T00:02:14.945386Z",
     "iopub.status.busy": "2022-10-15T00:02:14.944379Z",
     "iopub.status.idle": "2022-10-15T00:02:14.958376Z",
     "shell.execute_reply": "2022-10-15T00:02:14.958376Z",
     "shell.execute_reply.started": "2022-10-15T00:02:14.945386Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "emb_model = \"multi-qa-mpnet-base-dot-v1\" # \"all-mpnet-base-v2\" OR multi-qa-mpnet-base-dot-v1  OR \"xlm-r-bert-base-nli-stsb-mean-tokens\"\n",
    "model = BERTopic(embedding_model=emb_model, nr_topics=\"auto\", top_n_words=15) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58379d03-9447-4289-a3a2-a06679f1a960",
   "metadata": {},
   "source": [
    "#### Topics per doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56408c1-8ee7-4437-a5ed-328e0066aa0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-15T00:02:14.959378Z",
     "iopub.status.busy": "2022-10-15T00:02:14.959378Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bangladesh MDP.pdf STARTED\n"
     ]
    }
   ],
   "source": [
    "file_lst = data_df['file'].unique()\n",
    "\n",
    "for i in file_lst:\n",
    "    print(i, \"STARTED\")\n",
    "    # select only text from one pdf\n",
    "    docs = data_df[data_df['file']==i]['text'].dropna().tolist()\n",
    "    # estiamte topics and probabilities\n",
    "    topics, probabilities = model.fit_transform(docs)\n",
    "    # get topic counts and probabilities in one dataframe\n",
    "    topic_prob_df = pd.DataFrame([topics, probabilities]).T\n",
    "    topic_prob_mean_df = topic_prob_df.groupby([0]).mean(1).reset_index().sort_values(0)\n",
    "    topic_count_s = topic_prob_df[0].value_counts().sort_index()\n",
    "    topic_prob_mean_df['count'] = topic_count_s.to_list()\n",
    "    topic_prob_mean_df.columns = ['topic', 'probability', 'count']\n",
    "    # add topic words column\n",
    "    topic_df = pd.DataFrame(np.array(list(model.topic_representations_.values()))[:,:,0])\n",
    "    topic_df['topic_words'] = topic_df[topic_df.columns].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "    topic_df['topic'] = list(model.topic_representations_)\n",
    "    topic_df['topic_words'][0] = None\n",
    "    topic_df = topic_df[['topic', 'topic_words']]\n",
    "    topic_prob_mean_df = topic_prob_mean_df.merge(topic_df,how='left', on='topic')\n",
    "    topic_prob_mean_df.to_csv(i[:-4]+'_'+emb_model+'_topics.csv', index=False)\n",
    "    # create visualization\n",
    "    fig = model.visualize_topics()\n",
    "    fig.write_html(i[:-4]+'_'+emb_model+'_topics.html')\n",
    "    print(i, \"DONE\", \"\\n_________________\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8901a754-ad19-41cd-b789-5a37194dc367",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2bc53e-03f2-4a01-bf7e-a8c9d9bd2b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d971540b-e7cc-460e-97ae-26d9f7c6c69b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data_df['topic'] = topics\n",
    "# data_df['topic_prob'] = probabilities.tolist()\n",
    "\n",
    "# # add topic words column\n",
    "# topic_df = pd.DataFrame(np.array(list(model.topic_representations_.values()))[:,:,0])\n",
    "# topic_df['topic_words'] = topic_df[topic_df.columns].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "# topic_df['topic'] = list(model.topic_representations_)\n",
    "# topic_df['topic_words'][0] = None\n",
    "# topic_df = topic_df[['topic', 'topic_words']]\n",
    "# data_df.merge(topic_df,how='left', on='topic')\n",
    "\n",
    "# data_df.to_csv('papers_bertopic.csv', index=False)\n",
    "# data_df.to_feather('papers_bertopic.feather', index=False)\n",
    "\n",
    "# model.get_topic_info()\n",
    "\n",
    "# fig = model.visualize_topics()\n",
    "# fig.write_html(\"topics_viz.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4a8d8d-e80d-4071-8c07-bf57b445a4cf",
   "metadata": {},
   "source": [
    "#### Topics over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea007dd1-f398-421a-9598-572d3a225864",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# topics_over_time = model.topics_over_time(docs, timestamps=data_df['Year'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f595e49c-6093-4cbc-9374-32af8f47a695",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fig = model.visualize_topics_over_time(topics_over_time)\n",
    "# fig.write_html(\"topics_over_time.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28348b13-f9b0-4e54-a6a3-f0cd26f5ee3d",
   "metadata": {},
   "source": [
    "#### Multiple topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63274f84-a50a-424a-a2cb-b10a0cc80606",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model = BERTopic(embedding_model=\"all-mpnet-base-v2\", nr_topics=\"auto\", top_n_words=5) #\"xlm-r-bert-base-nli-stsb-mean-tokens\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042d7544-e4d0-42d0-9751-06897e7e998d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# num_repeats = 5\n",
    "# for i in range(num_repeats):\n",
    "#     topics, probabilities = model.fit_transform(docs)\n",
    "#     data_df['topic_'+str(i)] = topics\n",
    "#     data_df['topic_prob_'+str(i)] = probabilities.tolist()\n",
    "\n",
    "#     # add topic words column\n",
    "#     topic_df = pd.DataFrame(np.array(list(model.topic_representations_.values()))[:,:,0])\n",
    "#     topic_df['topic_words_'+str(i)] = topic_df[topic_df.columns].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "#     topic_df['topic'] = list(model.topic_representations_)\n",
    "#     topic_df['topic_words_'+str(i)][0] = None\n",
    "#     topic_df = topic_df[['topic', 'topic_words_'+str(i)]]\n",
    "#     data_df = data_df.merge(topic_df,how='left', left_on='topic_'+str(i), right_on='topic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f85d69-2bd2-43db-ae42-e59e46097bf0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data_df.to_csv('papers_bertopics_multi.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
