{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "963c8478-f33a-4217-86ea-3c42911c0b1f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-10-13T13:38:27.355338Z",
     "iopub.status.busy": "2022-10-13T13:38:27.354839Z",
     "iopub.status.idle": "2022-10-13T13:38:31.790980Z",
     "shell.execute_reply": "2022-10-13T13:38:31.789979Z",
     "shell.execute_reply.started": "2022-10-13T13:38:27.354839Z"
    },
    "id": "BrcXpPzTSszu",
    "outputId": "4af118e9-57bc-4313-f930-9c2348d2f219",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\GCM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Add rebel component https://github.com/Babelscape/rebel/blob/main/spacy_component.py\n",
    "import spacy\n",
    "import crosslingual_coreference\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "from spacy import Language\n",
    "from typing import List\n",
    "from spacy.tokens import Doc, Span\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63af62f8-d9d2-43f8-8464-f73aaee85eb6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-13T13:38:31.792979Z",
     "iopub.status.busy": "2022-10-13T13:38:31.792479Z",
     "iopub.status.idle": "2022-10-13T13:38:31.821481Z",
     "shell.execute_reply": "2022-10-13T13:38:31.820979Z",
     "shell.execute_reply.started": "2022-10-13T13:38:31.792479Z"
    },
    "id": "cGsgtd8lxhVo",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def call_wiki_api(item):\n",
    "    try:\n",
    "        url = f\"https://www.wikidata.org/w/api.php?action=wbsearchentities&search={item}&language=en&format=json\"\n",
    "        data = requests.get(url).json()\n",
    "        # Return the first id (Could upgrade this in the future)\n",
    "        return data['search'][0]['id']\n",
    "    except:\n",
    "        return 'id-less'\n",
    "\n",
    "def extract_triplets(text):\n",
    "    \"\"\"\n",
    "    Function to parse the generated text and extract the triplets\n",
    "    \"\"\"\n",
    "    triplets = []\n",
    "    relation, subject, relation, object_ = '', '', '', ''\n",
    "    text = text.strip()\n",
    "    current = 'x'\n",
    "    for token in text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\").split():\n",
    "        if token == \"<triplet>\":\n",
    "            current = 't'\n",
    "            if relation != '':\n",
    "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "                relation = ''\n",
    "            subject = ''\n",
    "        elif token == \"<subj>\":\n",
    "            current = 's'\n",
    "            if relation != '':\n",
    "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "            object_ = ''\n",
    "        elif token == \"<obj>\":\n",
    "            current = 'o'\n",
    "            relation = ''\n",
    "        else:\n",
    "            if current == 't':\n",
    "                subject += ' ' + token\n",
    "            elif current == 's':\n",
    "                object_ += ' ' + token\n",
    "            elif current == 'o':\n",
    "                relation += ' ' + token\n",
    "    if subject != '' and relation != '' and object_ != '':\n",
    "        triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "\n",
    "    return triplets\n",
    "\n",
    "\n",
    "@Language.factory(\n",
    "    \"rebel\",\n",
    "    requires=[\"doc.sents\"],\n",
    "    assigns=[\"doc._.rel\"],\n",
    "    default_config={\n",
    "        \"model_name\": \"Babelscape/rebel-large\",\n",
    "        \"device\": 0,\n",
    "    },\n",
    ")\n",
    "class RebelComponent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        nlp,\n",
    "        name,\n",
    "        model_name: str,\n",
    "        device: int,\n",
    "    ):\n",
    "        assert model_name is not None, \"\"\n",
    "        self.triplet_extractor = pipeline(\"text2text-generation\", model=model_name, tokenizer=model_name, device=device)\n",
    "        self.entity_mapping = {}\n",
    "        # Register custom extension on the Doc\n",
    "        if not Doc.has_extension(\"rel\"):\n",
    "            Doc.set_extension(\"rel\", default={})\n",
    "\n",
    "    def get_wiki_id(self, item: str):\n",
    "        mapping = self.entity_mapping.get(item)\n",
    "        if mapping:\n",
    "            return mapping\n",
    "        else:\n",
    "            res = call_wiki_api(item)\n",
    "            self.entity_mapping[item] = res\n",
    "            return res\n",
    "\n",
    "    \n",
    "    def _generate_triplets(self, sent: Span) -> List[dict]:\n",
    "        output_ids = self.triplet_extractor(sent.text, return_tensors=True, return_text=False)[0][\"generated_token_ids\"][\"output_ids\"]\n",
    "        extracted_text = self.triplet_extractor.tokenizer.batch_decode(output_ids[0])\n",
    "        extracted_triplets = extract_triplets(extracted_text[0])\n",
    "        return extracted_triplets\n",
    "\n",
    "    def set_annotations(self, doc: Doc, triplets: List[dict]):\n",
    "        for triplet in triplets:\n",
    "\n",
    "            # Remove self-loops (relationships that start and end at the entity)\n",
    "            if triplet['head'] == triplet['tail']:\n",
    "                continue\n",
    "\n",
    "            # Use regex to search for entities\n",
    "            head_span = re.search(triplet[\"head\"], doc.text)\n",
    "            tail_span = re.search(triplet[\"tail\"], doc.text)\n",
    "\n",
    "            # Skip the relation if both head and tail entities are not present in the text\n",
    "            # Sometimes the Rebel model hallucinates some entities\n",
    "            if not head_span or not tail_span:\n",
    "                continue\n",
    "\n",
    "            index = hashlib.sha1(\"\".join([triplet['head'], triplet['tail'], triplet['type']]).encode('utf-8')).hexdigest()\n",
    "            if index not in doc._.rel:\n",
    "                # Get wiki ids and store results\n",
    "                doc._.rel[index] = {\"relation\": triplet[\"type\"], \"head_span\": {'text': triplet['head'], 'id': self.get_wiki_id(triplet['head'])}, \"tail_span\": {'text': triplet['tail'], 'id': self.get_wiki_id(triplet['tail'])}}\n",
    "\n",
    "    def __call__(self, doc: Doc) -> Doc:\n",
    "        for sent in doc.sents:\n",
    "            sentence_triplets = self._generate_triplets(sent)\n",
    "            self.set_annotations(doc, sentence_triplets)\n",
    "        return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1241c1d-d664-4897-9af5-87facc47b718",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-10-13T13:38:31.822479Z",
     "iopub.status.busy": "2022-10-13T13:38:31.822479Z",
     "iopub.status.idle": "2022-10-13T13:38:54.954293Z",
     "shell.execute_reply": "2022-10-13T13:38:54.953291Z",
     "shell.execute_reply.started": "2022-10-13T13:38:31.822479Z"
    },
    "id": "SSTDfj3nSSca",
    "outputId": "0dbdbb3c-7a6c-46a3-8b9e-dc89f8af7486",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\GCM\\AppData\\Local\\Temp\\tmp39_md_v9\\config.json as plain json\n",
      "Some weights of the model checkpoint at nreimers/mMiniLMv2-L12-H384-distilled-from-XLMR-Large were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at nreimers/mMiniLMv2-L12-H384-distilled-from-XLMR-Large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.RebelComponent at 0x25511426d60>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = 0 # Number of the GPU, -1 if want to use CPU\n",
    "\n",
    "# Add coreference resolution model\n",
    "coref = spacy.load('en_core_web_sm', disable=['ner', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer'])\n",
    "coref.add_pipe(\"xx_coref\", \n",
    "    config={\"chunk_size\": 2500, \n",
    "            \"chunk_overlap\": 2, \n",
    "            \"device\": DEVICE}\n",
    "              )\n",
    "\n",
    "# Define rel extraction model\n",
    "rel_ext = spacy.load('en_core_web_sm', disable=['ner', 'lemmatizer', 'attribute_rules', 'tagger'])\n",
    "rel_ext.add_pipe(\"rebel\", \n",
    "                 config={\n",
    "                     'device':DEVICE,\n",
    "                     'model_name':'Babelscape/rebel-large'} # Model used, will default to 'Babelscape/rebel-large' if not given\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5b0524a-a834-4e5f-995a-e20c9f5a2b97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-13T13:38:54.955791Z",
     "iopub.status.busy": "2022-10-13T13:38:54.955292Z",
     "iopub.status.idle": "2022-10-13T13:38:54.969291Z",
     "shell.execute_reply": "2022-10-13T13:38:54.968792Z",
     "shell.execute_reply.started": "2022-10-13T13:38:54.955292Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# crosslingual_coreference implementation\n",
    "def coref_res(text_series, min_words=3):\n",
    "    coref_text_series = text_series.apply(lambda x : coref(x)._.resolved_text if len(x.split())>min_words else x)\n",
    "    return(coref_text_series)\n",
    "\n",
    "# # choose minilm for speed/memory and info_xlm for accuracy\n",
    "# predictor = Predictor(\n",
    "#     language=\"en_core_web_sm\", device=-1, model_name=\"minilm\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35c9b2b0-fc17-4219-a3f2-7814f71e1c33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-13T13:38:54.971299Z",
     "iopub.status.busy": "2022-10-13T13:38:54.970792Z",
     "iopub.status.idle": "2022-10-13T13:38:55.031292Z",
     "shell.execute_reply": "2022-10-13T13:38:55.030790Z",
     "shell.execute_reply.started": "2022-10-13T13:38:54.971299Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def link_entities_text(text):\n",
    "    try:\n",
    "        ent_rel_lst = list(rel_ext(text)._.rel.values())\n",
    "    except:\n",
    "        print(\"Could not extract relationships for text\")\n",
    "        ent_rel_lst = [{'relation': 'rel_err',\n",
    "                        'head_span': {'text': 'rel_err', 'id': 'rel_err'},\n",
    "                        'tail_span': {'text': 'rel_err', 'id': 'rel_err'}}]\n",
    "        \n",
    "    entity_df = pd.DataFrame()\n",
    "    rel_lst = []\n",
    "    head_text_lst = []\n",
    "    head_wiki_id_lst = []\n",
    "    tail_text_lst = []\n",
    "    tail_wiki_id_lst = []\n",
    "    for i in range(len(ent_rel_lst)):\n",
    "        rel_lst.append(ent_rel_lst[i]['relation'])\n",
    "        head_text_lst.append(ent_rel_lst[i]['head_span']['text'])\n",
    "        head_wiki_id_lst.append(ent_rel_lst[i]['head_span']['id'])\n",
    "        tail_text_lst.append(ent_rel_lst[i]['tail_span']['text'])\n",
    "        tail_wiki_id_lst.append(ent_rel_lst[i]['tail_span']['id'])\n",
    "    entity_df['head_text'] = head_text_lst\n",
    "    entity_df['head_wiki_id'] = head_wiki_id_lst\n",
    "    entity_df['relation'] = rel_lst\n",
    "    entity_df['tail_text'] = tail_text_lst\n",
    "    entity_df['tail_wiki_id'] = tail_wiki_id_lst\n",
    "    return(entity_df)\n",
    "\n",
    "def link_entities(text_series):\n",
    "    entity_df_series = text_series.apply(lambda x : link_entities_text(x))\n",
    "    return(entity_df_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "241dfc60-ff99-45d9-91cb-3638ff41236b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-13T13:38:55.032291Z",
     "iopub.status.busy": "2022-10-13T13:38:55.032291Z",
     "iopub.status.idle": "2022-10-13T13:38:55.341293Z",
     "shell.execute_reply": "2022-10-13T13:38:55.340790Z",
     "shell.execute_reply.started": "2022-10-13T13:38:55.032291Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data_df = pd.read_csv('E:\\\\GIT_REPOS\\\\LAB\\\\Literature_summary\\\\Test\\\\Entity_edgelist\\\\Input\\\\entomology-machine-learning-csv.csv')\n",
    "# data_df = data_df.drop('File', axis=1)\n",
    "# coref.max_length = 2612577\n",
    "# coref.max_length = 2612577\n",
    "pwd = os.getcwd()\n",
    "data_path = os.path.dirname(pwd) + \"\\\\Data\\\\NDPs\\\\\"\n",
    "data_df = pd.read_feather(data_path+\"docs.feather\")\n",
    "data_df = data_df.drop_duplicates().reset_index(drop=True)\n",
    "data_df[\"text\"] = data_df[\"text\"].astype(str)\n",
    "data_df['text'] = data_df['text'].str.split('\\n')\n",
    "data_df = data_df.explode('text').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bb6dc61-dd85-4543-b53b-e6906b3c0164",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-13T13:38:55.343293Z",
     "iopub.status.busy": "2022-10-13T13:38:55.343293Z",
     "iopub.status.idle": "2022-10-13T13:38:55.372793Z",
     "shell.execute_reply": "2022-10-13T13:38:55.371791Z",
     "shell.execute_reply.started": "2022-10-13T13:38:55.343293Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Bangladesh MDP.pdf', 'Bangladesh NDP.pdf', 'Bangladesh VNR.pdf',\n",
       "       'Botswana NDP.pdf', 'Botswana VNR.pdf', 'Cameroon MDP.pdf',\n",
       "       'Cameroon NDP.pdf', 'Eswatini NDP.pdf', 'Eswatini VNR.pdf',\n",
       "       'Gambia NDP.pdf', 'Gambia VNR.pdf', 'Ghana MDP.pdf',\n",
       "       'Ghana NDP.pdf', 'Ghana VNR.pdf', 'Kenya MDP.pdf', 'Kenya NDP.pdf',\n",
       "       'Kenya VNR.pdf', 'Lao NDP.pdf', 'Lao VNR.pdf', 'Liberia MDP.pdf',\n",
       "       'Liberia NDP.pdf', 'Liberia VNR.pdf', 'Malawi NDP.pdf',\n",
       "       'Malawi VNR.pdf', 'Namibia MDP.pdf', 'Namibia NDP.pdf',\n",
       "       'Namibia VNR.pdf', 'Nigeria NDP.pdf', 'Nigeria VNR.pdf',\n",
       "       'Pakistan NDP.pdf', 'Pakistan VNR.pdf', 'Rwanda NDP.pdf',\n",
       "       'Rwanda VNR.pdf', 'South Africa NDP.pdf', 'South Africa VNR.pdf',\n",
       "       'Soutn Africa MDP.pdf', 'Sri Lanka NDP.pdf', 'Sri Lanka VNR.pdf',\n",
       "       'Tanzania MDP.pdf', 'Tanzania NDP.pdf', 'Togo NDP.pdf',\n",
       "       'Togo VNR.pdf', 'Zambia NDP.pdf', 'Zambia VNR.pdf',\n",
       "       'Zimbabwe NDP.pdf', 'Zimbabwe VNR.pdf'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df['file'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3e4bb9c-de8f-4416-9523-97f926a1b739",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-13T13:38:55.373793Z",
     "iopub.status.busy": "2022-10-13T13:38:55.373793Z",
     "iopub.status.idle": "2022-10-13T13:38:55.434293Z",
     "shell.execute_reply": "2022-10-13T13:38:55.433790Z",
     "shell.execute_reply.started": "2022-10-13T13:38:55.373793Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Gambia VNR.pdf', 'Botswana NDP.pdf', 'Cameroon MDP.pdf']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "['Gambia VNR.pdf','Botswana NDP.pdf','Cameroon MDP.pdf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c346990e-cb18-4614-b442-9d964a98f345",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-13T13:38:55.435790Z",
     "iopub.status.busy": "2022-10-13T13:38:55.435293Z",
     "iopub.status.idle": "2022-10-13T13:38:55.511167Z",
     "shell.execute_reply": "2022-10-13T13:38:55.510664Z",
     "shell.execute_reply.started": "2022-10-13T13:38:55.435293Z"
    }
   },
   "outputs": [],
   "source": [
    "file_lst = [ 'Ghana MDP.pdf',\n",
    "       'Ghana NDP.pdf', 'Ghana VNR.pdf', 'Kenya MDP.pdf', 'Kenya NDP.pdf',\n",
    "       'Kenya VNR.pdf', 'Lao NDP.pdf', 'Lao VNR.pdf', 'Liberia MDP.pdf',\n",
    "       'Liberia NDP.pdf', 'Liberia VNR.pdf', 'Malawi NDP.pdf',\n",
    "       'Malawi VNR.pdf', 'Namibia MDP.pdf', 'Namibia NDP.pdf',\n",
    "       'Namibia VNR.pdf', 'Nigeria NDP.pdf', 'Nigeria VNR.pdf',\n",
    "       'Pakistan NDP.pdf', 'Pakistan VNR.pdf', 'Rwanda NDP.pdf',\n",
    "       'Rwanda VNR.pdf', 'South Africa NDP.pdf', 'South Africa VNR.pdf',\n",
    "       'Soutn Africa MDP.pdf', 'Sri Lanka NDP.pdf', 'Sri Lanka VNR.pdf',\n",
    "       'Tanzania MDP.pdf', 'Tanzania NDP.pdf', 'Togo NDP.pdf',\n",
    "       'Togo VNR.pdf', 'Zambia NDP.pdf', 'Zambia VNR.pdf',\n",
    "       'Zimbabwe NDP.pdf', 'Zimbabwe VNR.pdf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c30535-9ba8-4ce7-aaa1-16737cef26b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-13T13:38:55.512165Z",
     "iopub.status.busy": "2022-10-13T13:38:55.512165Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_______________________________\n",
      "\n",
      " Ghana MDP.pdf\n",
      "coref done 0 - 100\n",
      "entity linking done 0 - 100\n",
      "df create done 0 - 100\n",
      "df to list done 0 - 100 \n",
      "\n",
      "coref done 100 - 200\n",
      "entity linking done 100 - 200\n",
      "df create done 100 - 200\n",
      "df to list done 100 - 200 \n",
      "\n",
      "coref done 200 - 300\n",
      "entity linking done 200 - 300\n",
      "df create done 200 - 300\n",
      "df to list done 200 - 300 \n",
      "\n",
      "coref done 300 - 400\n",
      "entity linking done 300 - 400\n",
      "df create done 300 - 400\n",
      "df to list done 300 - 400 \n",
      "\n",
      "coref done 400 - 500\n",
      "entity linking done 400 - 500\n",
      "df create done 400 - 500\n",
      "df to list done 400 - 500 \n",
      "\n",
      "coref done 500 - 600\n",
      "entity linking done 500 - 600\n",
      "df create done 500 - 600\n",
      "df to list done 500 - 600 \n",
      "\n",
      "coref done 600 - 700\n",
      "entity linking done 600 - 700\n",
      "df create done 600 - 700\n",
      "df to list done 600 - 700 \n",
      "\n",
      "coref done 700 - 800\n",
      "entity linking done 700 - 800\n",
      "df create done 700 - 800\n",
      "df to list done 700 - 800 \n",
      "\n",
      "coref done 800 - 900\n",
      "entity linking done 800 - 900\n",
      "df create done 800 - 900\n",
      "df to list done 800 - 900 \n",
      "\n",
      "coref done 900 - 1000\n",
      "entity linking done 900 - 1000\n",
      "df create done 900 - 1000\n",
      "df to list done 900 - 1000 \n",
      "\n",
      "coref done 1000 - 1100\n",
      "entity linking done 1000 - 1100\n",
      "df create done 1000 - 1100\n",
      "df to list done 1000 - 1100 \n",
      "\n",
      "coref done 1100 - 1200\n",
      "entity linking done 1100 - 1200\n",
      "df create done 1100 - 1200\n",
      "df to list done 1100 - 1200 \n",
      "\n",
      "coref done 1200 - 1300\n",
      "entity linking done 1200 - 1300\n",
      "df create done 1200 - 1300\n",
      "df to list done 1200 - 1300 \n",
      "\n",
      "coref done 1300 - 1400\n",
      "entity linking done 1300 - 1400\n",
      "df create done 1300 - 1400\n",
      "df to list done 1300 - 1400 \n",
      "\n",
      "coref done 1400 - 1500\n",
      "entity linking done 1400 - 1500\n",
      "df create done 1400 - 1500\n",
      "df to list done 1400 - 1500 \n",
      "\n",
      "coref done 1500 - 1600\n",
      "entity linking done 1500 - 1600\n",
      "df create done 1500 - 1600\n",
      "df to list done 1500 - 1600 \n",
      "\n",
      "coref done 1600 - 1700\n",
      "entity linking done 1600 - 1700\n",
      "df create done 1600 - 1700\n",
      "df to list done 1600 - 1700 \n",
      "\n",
      "coref done 1700 - 1800\n",
      "entity linking done 1700 - 1800\n",
      "df create done 1700 - 1800\n",
      "df to list done 1700 - 1800 \n",
      "\n",
      "coref done 1800 - 1900\n",
      "entity linking done 1800 - 1900\n",
      "df create done 1800 - 1900\n",
      "df to list done 1800 - 1900 \n",
      "\n",
      "coref done 1900 - 2000\n",
      "entity linking done 1900 - 2000\n",
      "df create done 1900 - 2000\n",
      "df to list done 1900 - 2000 \n",
      "\n",
      "coref done 2000 - 2100\n",
      "entity linking done 2000 - 2100\n",
      "df create done 2000 - 2100\n",
      "df to list done 2000 - 2100 \n",
      "\n",
      "coref done 2100 - 2200\n",
      "entity linking done 2100 - 2200\n",
      "df create done 2100 - 2200\n",
      "df to list done 2100 - 2200 \n",
      "\n",
      "coref done 2200 - 2300\n",
      "entity linking done 2200 - 2300\n",
      "df create done 2200 - 2300\n",
      "df to list done 2200 - 2300 \n",
      "\n",
      "coref done 2300 - 2400\n",
      "entity linking done 2300 - 2400\n",
      "df create done 2300 - 2400\n",
      "df to list done 2300 - 2400 \n",
      "\n",
      "coref done 2400 - 2500\n",
      "entity linking done 2400 - 2500\n",
      "df create done 2400 - 2500\n",
      "df to list done 2400 - 2500 \n",
      "\n",
      "coref done 2500 - 2600\n",
      "entity linking done 2500 - 2600\n",
      "df create done 2500 - 2600\n",
      "df to list done 2500 - 2600 \n",
      "\n",
      "coref done 2600 - 2700\n",
      "entity linking done 2600 - 2700\n",
      "df create done 2600 - 2700\n",
      "df to list done 2600 - 2700 \n",
      "\n",
      "coref done 2700 - 2800\n",
      "entity linking done 2700 - 2800\n",
      "df create done 2700 - 2800\n",
      "df to list done 2700 - 2800 \n",
      "\n",
      "coref done 2800 - 2900\n",
      "entity linking done 2800 - 2900\n",
      "df create done 2800 - 2900\n",
      "df to list done 2800 - 2900 \n",
      "\n",
      "coref done 2900 - 3000\n",
      "entity linking done 2900 - 3000\n",
      "df create done 2900 - 3000\n",
      "df to list done 2900 - 3000 \n",
      "\n",
      "coref done 3000 - 3100\n",
      "entity linking done 3000 - 3100\n",
      "df create done 3000 - 3100\n",
      "df to list done 3000 - 3100 \n",
      "\n",
      "coref done 3100 - 3200\n",
      "entity linking done 3100 - 3200\n",
      "df create done 3100 - 3200\n",
      "df to list done 3100 - 3200 \n",
      "\n",
      "_______________________________\n",
      "\n",
      " Ghana NDP.pdf\n",
      "coref done 0 - 100\n"
     ]
    }
   ],
   "source": [
    "# Calculate and save per paper\n",
    "win_size = 100\n",
    "start_point = 0\n",
    "entities_df_lst = []\n",
    "for j in file_lst:\n",
    "    print('_______________________________\\n\\n', j)\n",
    "    df = data_df[data_df['file']==j]\n",
    "    for i in range(start_point, len(df), win_size):\n",
    "        coref_series = coref_res(text_series=df[\"text\"].iloc[i:i+win_size])\n",
    "        print('coref done', i, '-', i+win_size)\n",
    "        link_entities_series = link_entities(text_series=coref_series)\n",
    "        print('entity linking done', i, '-', i+win_size)\n",
    "        entities_df = pd.concat(link_entities_series.tolist())\n",
    "        print('df create done', i, '-', i+win_size)\n",
    "        entities_df_lst.append(entities_df)\n",
    "        print('df to list done', i, '-', i+win_size, '\\n')\n",
    "    all_entities_df = pd.concat(entities_df_lst)\n",
    "    all_entities_df.reset_index(drop=True, inplace=True)\n",
    "    edge_lst_df = all_entities_df.value_counts().reset_index().rename(columns={0: \"count\"})\n",
    "    edge_lst_df.to_csv('entity_weighted_edgelist_ALL_'+j[:-4]+'.csv')\n",
    "    edge_lst_df.to_feather('entity_weighted_edgelist_ALL_'+j[:-4]+'.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d36c245-8c59-4c34-8ec0-f4661faf0cf6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# win_size = 1000\n",
    "# start_point = 0 #46000 # default to 0\n",
    "# entities_df_lst = []\n",
    "# for i in range(start_point, len(data_df), win_size):\n",
    "#     coref_series = coref_res(text_series=data_df[\"text\"].iloc[i:i+win_size])\n",
    "#     print('coref done', i, '-', i+win_size)\n",
    "#     link_entities_series = link_entities(text_series=coref_series)\n",
    "#     print('entity linking done', i, '-', i+win_size)\n",
    "#     entities_df = pd.concat(link_entities_series.tolist())\n",
    "#     print('df create done', i, '-', i+win_size)\n",
    "#     entities_df_lst.append(entities_df)\n",
    "#     print('df to list done', i, '-', i+win_size, '\\n')\n",
    "# all_entities_df = pd.concat(entities_df_lst)\n",
    "# # all_entities_df.reset_index(drop=True, inplace=True)\n",
    "# edge_lst_df = all_entities_df.value_counts().reset_index().rename(columns={0: \"count\"})\n",
    "# edge_lst_df.to_csv('entity_weighted_edgelist_ALL.csv')\n",
    "# edge_lst_df.to_feather('entity_weighted_edgelist_ALL.feather')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
