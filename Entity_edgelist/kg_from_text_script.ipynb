{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "963c8478-f33a-4217-86ea-3c42911c0b1f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-10-12T13:39:37.269357Z",
     "iopub.status.busy": "2022-10-12T13:39:37.268860Z",
     "iopub.status.idle": "2022-10-12T13:39:41.934769Z",
     "shell.execute_reply": "2022-10-12T13:39:41.933765Z",
     "shell.execute_reply.started": "2022-10-12T13:39:37.269357Z"
    },
    "id": "BrcXpPzTSszu",
    "outputId": "4af118e9-57bc-4313-f930-9c2348d2f219",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\GCM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Add rebel component https://github.com/Babelscape/rebel/blob/main/spacy_component.py\n",
    "import spacy\n",
    "import crosslingual_coreference\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "from spacy import Language\n",
    "from typing import List\n",
    "from spacy.tokens import Doc, Span\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63af62f8-d9d2-43f8-8464-f73aaee85eb6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-12T13:39:41.937265Z",
     "iopub.status.busy": "2022-10-12T13:39:41.936766Z",
     "iopub.status.idle": "2022-10-12T13:39:41.965772Z",
     "shell.execute_reply": "2022-10-12T13:39:41.964766Z",
     "shell.execute_reply.started": "2022-10-12T13:39:41.937265Z"
    },
    "id": "cGsgtd8lxhVo",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def call_wiki_api(item):\n",
    "    try:\n",
    "        url = f\"https://www.wikidata.org/w/api.php?action=wbsearchentities&search={item}&language=en&format=json\"\n",
    "        data = requests.get(url).json()\n",
    "        # Return the first id (Could upgrade this in the future)\n",
    "        return data['search'][0]['id']\n",
    "    except:\n",
    "        return 'id-less'\n",
    "\n",
    "def extract_triplets(text):\n",
    "    \"\"\"\n",
    "    Function to parse the generated text and extract the triplets\n",
    "    \"\"\"\n",
    "    triplets = []\n",
    "    relation, subject, relation, object_ = '', '', '', ''\n",
    "    text = text.strip()\n",
    "    current = 'x'\n",
    "    for token in text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\").split():\n",
    "        if token == \"<triplet>\":\n",
    "            current = 't'\n",
    "            if relation != '':\n",
    "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "                relation = ''\n",
    "            subject = ''\n",
    "        elif token == \"<subj>\":\n",
    "            current = 's'\n",
    "            if relation != '':\n",
    "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "            object_ = ''\n",
    "        elif token == \"<obj>\":\n",
    "            current = 'o'\n",
    "            relation = ''\n",
    "        else:\n",
    "            if current == 't':\n",
    "                subject += ' ' + token\n",
    "            elif current == 's':\n",
    "                object_ += ' ' + token\n",
    "            elif current == 'o':\n",
    "                relation += ' ' + token\n",
    "    if subject != '' and relation != '' and object_ != '':\n",
    "        triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "\n",
    "    return triplets\n",
    "\n",
    "\n",
    "@Language.factory(\n",
    "    \"rebel\",\n",
    "    requires=[\"doc.sents\"],\n",
    "    assigns=[\"doc._.rel\"],\n",
    "    default_config={\n",
    "        \"model_name\": \"Babelscape/rebel-large\",\n",
    "        \"device\": 0,\n",
    "    },\n",
    ")\n",
    "class RebelComponent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        nlp,\n",
    "        name,\n",
    "        model_name: str,\n",
    "        device: int,\n",
    "    ):\n",
    "        assert model_name is not None, \"\"\n",
    "        self.triplet_extractor = pipeline(\"text2text-generation\", model=model_name, tokenizer=model_name, device=device)\n",
    "        self.entity_mapping = {}\n",
    "        # Register custom extension on the Doc\n",
    "        if not Doc.has_extension(\"rel\"):\n",
    "            Doc.set_extension(\"rel\", default={})\n",
    "\n",
    "    def get_wiki_id(self, item: str):\n",
    "        mapping = self.entity_mapping.get(item)\n",
    "        if mapping:\n",
    "            return mapping\n",
    "        else:\n",
    "            res = call_wiki_api(item)\n",
    "            self.entity_mapping[item] = res\n",
    "            return res\n",
    "\n",
    "    \n",
    "    def _generate_triplets(self, sent: Span) -> List[dict]:\n",
    "        output_ids = self.triplet_extractor(sent.text, return_tensors=True, return_text=False)[0][\"generated_token_ids\"][\"output_ids\"]\n",
    "        extracted_text = self.triplet_extractor.tokenizer.batch_decode(output_ids[0])\n",
    "        extracted_triplets = extract_triplets(extracted_text[0])\n",
    "        return extracted_triplets\n",
    "\n",
    "    def set_annotations(self, doc: Doc, triplets: List[dict]):\n",
    "        for triplet in triplets:\n",
    "\n",
    "            # Remove self-loops (relationships that start and end at the entity)\n",
    "            if triplet['head'] == triplet['tail']:\n",
    "                continue\n",
    "\n",
    "            # Use regex to search for entities\n",
    "            head_span = re.search(triplet[\"head\"], doc.text)\n",
    "            tail_span = re.search(triplet[\"tail\"], doc.text)\n",
    "\n",
    "            # Skip the relation if both head and tail entities are not present in the text\n",
    "            # Sometimes the Rebel model hallucinates some entities\n",
    "            if not head_span or not tail_span:\n",
    "                continue\n",
    "\n",
    "            index = hashlib.sha1(\"\".join([triplet['head'], triplet['tail'], triplet['type']]).encode('utf-8')).hexdigest()\n",
    "            if index not in doc._.rel:\n",
    "                # Get wiki ids and store results\n",
    "                doc._.rel[index] = {\"relation\": triplet[\"type\"], \"head_span\": {'text': triplet['head'], 'id': self.get_wiki_id(triplet['head'])}, \"tail_span\": {'text': triplet['tail'], 'id': self.get_wiki_id(triplet['tail'])}}\n",
    "\n",
    "    def __call__(self, doc: Doc) -> Doc:\n",
    "        for sent in doc.sents:\n",
    "            sentence_triplets = self._generate_triplets(sent)\n",
    "            self.set_annotations(doc, sentence_triplets)\n",
    "        return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1241c1d-d664-4897-9af5-87facc47b718",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-10-12T13:39:41.967267Z",
     "iopub.status.busy": "2022-10-12T13:39:41.967267Z",
     "iopub.status.idle": "2022-10-12T13:40:05.723697Z",
     "shell.execute_reply": "2022-10-12T13:40:05.722694Z",
     "shell.execute_reply.started": "2022-10-12T13:39:41.967267Z"
    },
    "id": "SSTDfj3nSSca",
    "outputId": "0dbdbb3c-7a6c-46a3-8b9e-dc89f8af7486",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\GCM\\AppData\\Local\\Temp\\tmpmfgwld4y\\config.json as plain json\n",
      "Some weights of the model checkpoint at nreimers/mMiniLMv2-L12-H384-distilled-from-XLMR-Large were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at nreimers/mMiniLMv2-L12-H384-distilled-from-XLMR-Large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.RebelComponent at 0x27f16c26fd0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = 0 # Number of the GPU, -1 if want to use CPU\n",
    "\n",
    "# Add coreference resolution model\n",
    "coref = spacy.load('en_core_web_sm', disable=['ner', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer'])\n",
    "coref.add_pipe(\"xx_coref\", \n",
    "    config={\"chunk_size\": 2500, \n",
    "            \"chunk_overlap\": 2, \n",
    "            \"device\": DEVICE}\n",
    "              )\n",
    "\n",
    "# Define rel extraction model\n",
    "rel_ext = spacy.load('en_core_web_sm', disable=['ner', 'lemmatizer', 'attribute_rules', 'tagger'])\n",
    "rel_ext.add_pipe(\"rebel\", \n",
    "                 config={\n",
    "                     'device':DEVICE,\n",
    "                     'model_name':'Babelscape/rebel-large'} # Model used, will default to 'Babelscape/rebel-large' if not given\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5b0524a-a834-4e5f-995a-e20c9f5a2b97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-12T13:40:05.725195Z",
     "iopub.status.busy": "2022-10-12T13:40:05.725195Z",
     "iopub.status.idle": "2022-10-12T13:40:05.739698Z",
     "shell.execute_reply": "2022-10-12T13:40:05.738195Z",
     "shell.execute_reply.started": "2022-10-12T13:40:05.725195Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# crosslingual_coreference implementation\n",
    "def coref_res(text_series, min_words=3):\n",
    "    coref_text_series = text_series.apply(lambda x : coref(x)._.resolved_text if len(x.split())>min_words else x)\n",
    "    return(coref_text_series)\n",
    "\n",
    "# # choose minilm for speed/memory and info_xlm for accuracy\n",
    "# predictor = Predictor(\n",
    "#     language=\"en_core_web_sm\", device=-1, model_name=\"minilm\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35c9b2b0-fc17-4219-a3f2-7814f71e1c33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-12T13:40:05.741700Z",
     "iopub.status.busy": "2022-10-12T13:40:05.741204Z",
     "iopub.status.idle": "2022-10-12T13:40:05.801340Z",
     "shell.execute_reply": "2022-10-12T13:40:05.800341Z",
     "shell.execute_reply.started": "2022-10-12T13:40:05.741700Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def link_entities_text(text):\n",
    "    try:\n",
    "        ent_rel_lst = list(rel_ext(text)._.rel.values())\n",
    "    except:\n",
    "        print(\"Could not extract relationships for text\")\n",
    "        ent_rel_lst = [{'relation': 'rel_err',\n",
    "                        'head_span': {'text': 'rel_err', 'id': 'rel_err'},\n",
    "                        'tail_span': {'text': 'rel_err', 'id': 'rel_err'}}]\n",
    "        \n",
    "    entity_df = pd.DataFrame()\n",
    "    rel_lst = []\n",
    "    head_text_lst = []\n",
    "    head_wiki_id_lst = []\n",
    "    tail_text_lst = []\n",
    "    tail_wiki_id_lst = []\n",
    "    for i in range(len(ent_rel_lst)):\n",
    "        rel_lst.append(ent_rel_lst[i]['relation'])\n",
    "        head_text_lst.append(ent_rel_lst[i]['head_span']['text'])\n",
    "        head_wiki_id_lst.append(ent_rel_lst[i]['head_span']['id'])\n",
    "        tail_text_lst.append(ent_rel_lst[i]['tail_span']['text'])\n",
    "        tail_wiki_id_lst.append(ent_rel_lst[i]['tail_span']['id'])\n",
    "    entity_df['head_text'] = head_text_lst\n",
    "    entity_df['head_wiki_id'] = head_wiki_id_lst\n",
    "    entity_df['relation'] = rel_lst\n",
    "    entity_df['tail_text'] = tail_text_lst\n",
    "    entity_df['tail_wiki_id'] = tail_wiki_id_lst\n",
    "    return(entity_df)\n",
    "\n",
    "def link_entities(text_series):\n",
    "    entity_df_series = text_series.apply(lambda x : link_entities_text(x))\n",
    "    return(entity_df_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "241dfc60-ff99-45d9-91cb-3638ff41236b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-12T13:40:05.802839Z",
     "iopub.status.busy": "2022-10-12T13:40:05.802340Z",
     "iopub.status.idle": "2022-10-12T13:40:06.127265Z",
     "shell.execute_reply": "2022-10-12T13:40:06.125767Z",
     "shell.execute_reply.started": "2022-10-12T13:40:05.802839Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data_df = pd.read_csv('E:\\\\GIT_REPOS\\\\LAB\\\\Literature_summary\\\\Test\\\\Entity_edgelist\\\\Input\\\\entomology-machine-learning-csv.csv')\n",
    "# data_df = data_df.drop('File', axis=1)\n",
    "# coref.max_length = 2612577\n",
    "# coref.max_length = 2612577\n",
    "pwd = os.getcwd()\n",
    "data_path = os.path.dirname(pwd) + \"\\\\Data\\\\NDPs\\\\\"\n",
    "data_df = pd.read_feather(data_path+\"docs.feather\")\n",
    "data_df = data_df.drop_duplicates().reset_index(drop=True)\n",
    "data_df[\"text\"] = data_df[\"text\"].astype(str)\n",
    "data_df['text'] = data_df['text'].str.split('\\n')\n",
    "data_df = data_df.explode('text').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c30535-9ba8-4ce7-aaa1-16737cef26b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-12T13:40:41.229356Z",
     "iopub.status.busy": "2022-10-12T13:40:41.228855Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bangladesh MDP.pdf\n",
      "coref done 0 - 100\n",
      "entity linking done 0 - 100\n",
      "df create done 0 - 100\n",
      "df to list done 0 - 100 \n",
      "\n",
      "coref done 100 - 200\n",
      "entity linking done 100 - 200\n",
      "df create done 100 - 200\n",
      "df to list done 100 - 200 \n",
      "\n",
      "coref done 200 - 300\n",
      "entity linking done 200 - 300\n",
      "df create done 200 - 300\n",
      "df to list done 200 - 300 \n",
      "\n",
      "coref done 300 - 400\n",
      "entity linking done 300 - 400\n",
      "df create done 300 - 400\n",
      "df to list done 300 - 400 \n",
      "\n",
      "coref done 400 - 500\n",
      "entity linking done 400 - 500\n",
      "df create done 400 - 500\n",
      "df to list done 400 - 500 \n",
      "\n",
      "coref done 500 - 600\n",
      "entity linking done 500 - 600\n",
      "df create done 500 - 600\n",
      "df to list done 500 - 600 \n",
      "\n",
      "coref done 600 - 700\n",
      "entity linking done 600 - 700\n",
      "df create done 600 - 700\n",
      "df to list done 600 - 700 \n",
      "\n",
      "coref done 700 - 800\n",
      "entity linking done 700 - 800\n",
      "df create done 700 - 800\n",
      "df to list done 700 - 800 \n",
      "\n",
      "coref done 800 - 900\n",
      "entity linking done 800 - 900\n",
      "df create done 800 - 900\n",
      "df to list done 800 - 900 \n",
      "\n",
      "coref done 900 - 1000\n",
      "entity linking done 900 - 1000\n",
      "df create done 900 - 1000\n",
      "df to list done 900 - 1000 \n",
      "\n",
      "coref done 1000 - 1100\n",
      "entity linking done 1000 - 1100\n",
      "df create done 1000 - 1100\n",
      "df to list done 1000 - 1100 \n",
      "\n",
      "coref done 1100 - 1200\n",
      "entity linking done 1100 - 1200\n",
      "df create done 1100 - 1200\n",
      "df to list done 1100 - 1200 \n",
      "\n",
      "coref done 1200 - 1300\n",
      "entity linking done 1200 - 1300\n",
      "df create done 1200 - 1300\n",
      "df to list done 1200 - 1300 \n",
      "\n",
      "coref done 1300 - 1400\n",
      "entity linking done 1300 - 1400\n",
      "df create done 1300 - 1400\n",
      "df to list done 1300 - 1400 \n",
      "\n",
      "coref done 1400 - 1500\n",
      "entity linking done 1400 - 1500\n",
      "df create done 1400 - 1500\n",
      "df to list done 1400 - 1500 \n",
      "\n",
      "coref done 1500 - 1600\n",
      "entity linking done 1500 - 1600\n",
      "df create done 1500 - 1600\n",
      "df to list done 1500 - 1600 \n",
      "\n",
      "coref done 1600 - 1700\n",
      "entity linking done 1600 - 1700\n",
      "df create done 1600 - 1700\n",
      "df to list done 1600 - 1700 \n",
      "\n",
      "coref done 1700 - 1800\n",
      "entity linking done 1700 - 1800\n",
      "df create done 1700 - 1800\n",
      "df to list done 1700 - 1800 \n",
      "\n",
      "coref done 1800 - 1900\n",
      "entity linking done 1800 - 1900\n",
      "df create done 1800 - 1900\n",
      "df to list done 1800 - 1900 \n",
      "\n",
      "coref done 1900 - 2000\n",
      "entity linking done 1900 - 2000\n",
      "df create done 1900 - 2000\n",
      "df to list done 1900 - 2000 \n",
      "\n",
      "coref done 2000 - 2100\n",
      "entity linking done 2000 - 2100\n",
      "df create done 2000 - 2100\n",
      "df to list done 2000 - 2100 \n",
      "\n",
      "coref done 2100 - 2200\n",
      "entity linking done 2100 - 2200\n",
      "df create done 2100 - 2200\n",
      "df to list done 2100 - 2200 \n",
      "\n",
      "coref done 2200 - 2300\n",
      "entity linking done 2200 - 2300\n",
      "df create done 2200 - 2300\n",
      "df to list done 2200 - 2300 \n",
      "\n",
      "coref done 2300 - 2400\n",
      "entity linking done 2300 - 2400\n",
      "df create done 2300 - 2400\n",
      "df to list done 2300 - 2400 \n",
      "\n",
      "coref done 2400 - 2500\n",
      "entity linking done 2400 - 2500\n",
      "df create done 2400 - 2500\n",
      "df to list done 2400 - 2500 \n",
      "\n",
      "coref done 2500 - 2600\n",
      "entity linking done 2500 - 2600\n",
      "df create done 2500 - 2600\n",
      "df to list done 2500 - 2600 \n",
      "\n",
      "coref done 2600 - 2700\n",
      "entity linking done 2600 - 2700\n",
      "df create done 2600 - 2700\n",
      "df to list done 2600 - 2700 \n",
      "\n",
      "coref done 2700 - 2800\n",
      "entity linking done 2700 - 2800\n",
      "df create done 2700 - 2800\n",
      "df to list done 2700 - 2800 \n",
      "\n",
      "coref done 2800 - 2900\n",
      "entity linking done 2800 - 2900\n",
      "df create done 2800 - 2900\n",
      "df to list done 2800 - 2900 \n",
      "\n",
      "coref done 2900 - 3000\n",
      "entity linking done 2900 - 3000\n",
      "df create done 2900 - 3000\n",
      "df to list done 2900 - 3000 \n",
      "\n",
      "coref done 3000 - 3100\n",
      "entity linking done 3000 - 3100\n",
      "df create done 3000 - 3100\n",
      "df to list done 3000 - 3100 \n",
      "\n",
      "coref done 3100 - 3200\n",
      "entity linking done 3100 - 3200\n",
      "df create done 3100 - 3200\n",
      "df to list done 3100 - 3200 \n",
      "\n",
      "coref done 3200 - 3300\n",
      "entity linking done 3200 - 3300\n",
      "df create done 3200 - 3300\n",
      "df to list done 3200 - 3300 \n",
      "\n",
      "coref done 3300 - 3400\n",
      "entity linking done 3300 - 3400\n",
      "df create done 3300 - 3400\n",
      "df to list done 3300 - 3400 \n",
      "\n",
      "coref done 3400 - 3500\n",
      "entity linking done 3400 - 3500\n",
      "df create done 3400 - 3500\n",
      "df to list done 3400 - 3500 \n",
      "\n",
      "coref done 3500 - 3600\n",
      "entity linking done 3500 - 3600\n",
      "df create done 3500 - 3600\n",
      "df to list done 3500 - 3600 \n",
      "\n",
      "coref done 3600 - 3700\n",
      "Could not extract relationships for text\n",
      "entity linking done 3600 - 3700\n",
      "df create done 3600 - 3700\n",
      "df to list done 3600 - 3700 \n",
      "\n",
      "coref done 3700 - 3800\n",
      "entity linking done 3700 - 3800\n",
      "df create done 3700 - 3800\n",
      "df to list done 3700 - 3800 \n",
      "\n",
      "coref done 3800 - 3900\n",
      "entity linking done 3800 - 3900\n",
      "df create done 3800 - 3900\n",
      "df to list done 3800 - 3900 \n",
      "\n",
      "coref done 3900 - 4000\n",
      "entity linking done 3900 - 4000\n",
      "df create done 3900 - 4000\n",
      "df to list done 3900 - 4000 \n",
      "\n",
      "coref done 4000 - 4100\n",
      "entity linking done 4000 - 4100\n",
      "df create done 4000 - 4100\n",
      "df to list done 4000 - 4100 \n",
      "\n",
      "coref done 4100 - 4200\n",
      "entity linking done 4100 - 4200\n",
      "df create done 4100 - 4200\n",
      "df to list done 4100 - 4200 \n",
      "\n",
      "coref done 4200 - 4300\n",
      "entity linking done 4200 - 4300\n",
      "df create done 4200 - 4300\n",
      "df to list done 4200 - 4300 \n",
      "\n",
      "coref done 4300 - 4400\n",
      "entity linking done 4300 - 4400\n",
      "df create done 4300 - 4400\n",
      "df to list done 4300 - 4400 \n",
      "\n",
      "coref done 4400 - 4500\n",
      "entity linking done 4400 - 4500\n",
      "df create done 4400 - 4500\n",
      "df to list done 4400 - 4500 \n",
      "\n",
      "coref done 4500 - 4600\n",
      "entity linking done 4500 - 4600\n",
      "df create done 4500 - 4600\n",
      "df to list done 4500 - 4600 \n",
      "\n",
      "coref done 4600 - 4700\n",
      "entity linking done 4600 - 4700\n",
      "df create done 4600 - 4700\n",
      "df to list done 4600 - 4700 \n",
      "\n",
      "coref done 4700 - 4800\n",
      "entity linking done 4700 - 4800\n",
      "df create done 4700 - 4800\n",
      "df to list done 4700 - 4800 \n",
      "\n",
      "coref done 4800 - 4900\n",
      "entity linking done 4800 - 4900\n",
      "df create done 4800 - 4900\n",
      "df to list done 4800 - 4900 \n",
      "\n",
      "coref done 4900 - 5000\n",
      "entity linking done 4900 - 5000\n",
      "df create done 4900 - 5000\n",
      "df to list done 4900 - 5000 \n",
      "\n",
      "coref done 5000 - 5100\n",
      "entity linking done 5000 - 5100\n",
      "df create done 5000 - 5100\n",
      "df to list done 5000 - 5100 \n",
      "\n",
      "coref done 5100 - 5200\n",
      "entity linking done 5100 - 5200\n",
      "df create done 5100 - 5200\n",
      "df to list done 5100 - 5200 \n",
      "\n",
      "coref done 5200 - 5300\n",
      "entity linking done 5200 - 5300\n",
      "df create done 5200 - 5300\n",
      "df to list done 5200 - 5300 \n",
      "\n",
      "coref done 5300 - 5400\n",
      "entity linking done 5300 - 5400\n",
      "df create done 5300 - 5400\n",
      "df to list done 5300 - 5400 \n",
      "\n",
      "coref done 5400 - 5500\n",
      "entity linking done 5400 - 5500\n",
      "df create done 5400 - 5500\n",
      "df to list done 5400 - 5500 \n",
      "\n",
      "coref done 5500 - 5600\n",
      "entity linking done 5500 - 5600\n",
      "df create done 5500 - 5600\n",
      "df to list done 5500 - 5600 \n",
      "\n",
      "coref done 5600 - 5700\n",
      "entity linking done 5600 - 5700\n",
      "df create done 5600 - 5700\n",
      "df to list done 5600 - 5700 \n",
      "\n",
      "coref done 5700 - 5800\n",
      "entity linking done 5700 - 5800\n",
      "df create done 5700 - 5800\n",
      "df to list done 5700 - 5800 \n",
      "\n",
      "coref done 5800 - 5900\n",
      "entity linking done 5800 - 5900\n",
      "df create done 5800 - 5900\n",
      "df to list done 5800 - 5900 \n",
      "\n",
      "coref done 5900 - 6000\n",
      "entity linking done 5900 - 6000\n",
      "df create done 5900 - 6000\n",
      "df to list done 5900 - 6000 \n",
      "\n",
      "coref done 6000 - 6100\n",
      "entity linking done 6000 - 6100\n",
      "df create done 6000 - 6100\n",
      "df to list done 6000 - 6100 \n",
      "\n",
      "coref done 6100 - 6200\n",
      "entity linking done 6100 - 6200\n",
      "df create done 6100 - 6200\n",
      "df to list done 6100 - 6200 \n",
      "\n",
      "coref done 6200 - 6300\n",
      "entity linking done 6200 - 6300\n",
      "df create done 6200 - 6300\n",
      "df to list done 6200 - 6300 \n",
      "\n",
      "coref done 6300 - 6400\n",
      "entity linking done 6300 - 6400\n",
      "df create done 6300 - 6400\n",
      "df to list done 6300 - 6400 \n",
      "\n",
      "coref done 6400 - 6500\n",
      "entity linking done 6400 - 6500\n",
      "df create done 6400 - 6500\n",
      "df to list done 6400 - 6500 \n",
      "\n",
      "coref done 6500 - 6600\n",
      "entity linking done 6500 - 6600\n",
      "df create done 6500 - 6600\n",
      "df to list done 6500 - 6600 \n",
      "\n",
      "coref done 6600 - 6700\n",
      "entity linking done 6600 - 6700\n",
      "df create done 6600 - 6700\n",
      "df to list done 6600 - 6700 \n",
      "\n",
      "coref done 6700 - 6800\n",
      "entity linking done 6700 - 6800\n",
      "df create done 6700 - 6800\n",
      "df to list done 6700 - 6800 \n",
      "\n",
      "coref done 6800 - 6900\n",
      "entity linking done 6800 - 6900\n",
      "df create done 6800 - 6900\n",
      "df to list done 6800 - 6900 \n",
      "\n",
      "coref done 6900 - 7000\n",
      "entity linking done 6900 - 7000\n",
      "df create done 6900 - 7000\n",
      "df to list done 6900 - 7000 \n",
      "\n",
      "coref done 7000 - 7100\n",
      "entity linking done 7000 - 7100\n",
      "df create done 7000 - 7100\n",
      "df to list done 7000 - 7100 \n",
      "\n",
      "coref done 7100 - 7200\n",
      "entity linking done 7100 - 7200\n",
      "df create done 7100 - 7200\n",
      "df to list done 7100 - 7200 \n",
      "\n",
      "coref done 7200 - 7300\n",
      "entity linking done 7200 - 7300\n",
      "df create done 7200 - 7300\n",
      "df to list done 7200 - 7300 \n",
      "\n",
      "coref done 7300 - 7400\n",
      "entity linking done 7300 - 7400\n",
      "df create done 7300 - 7400\n",
      "df to list done 7300 - 7400 \n",
      "\n",
      "coref done 7400 - 7500\n",
      "entity linking done 7400 - 7500\n",
      "df create done 7400 - 7500\n",
      "df to list done 7400 - 7500 \n",
      "\n",
      "coref done 7500 - 7600\n",
      "entity linking done 7500 - 7600\n",
      "df create done 7500 - 7600\n",
      "df to list done 7500 - 7600 \n",
      "\n",
      "coref done 7600 - 7700\n",
      "entity linking done 7600 - 7700\n",
      "df create done 7600 - 7700\n",
      "df to list done 7600 - 7700 \n",
      "\n",
      "coref done 7700 - 7800\n",
      "entity linking done 7700 - 7800\n",
      "df create done 7700 - 7800\n",
      "df to list done 7700 - 7800 \n",
      "\n",
      "coref done 7800 - 7900\n",
      "entity linking done 7800 - 7900\n",
      "df create done 7800 - 7900\n",
      "df to list done 7800 - 7900 \n",
      "\n",
      "coref done 7900 - 8000\n",
      "entity linking done 7900 - 8000\n",
      "df create done 7900 - 8000\n",
      "df to list done 7900 - 8000 \n",
      "\n",
      "coref done 8000 - 8100\n",
      "entity linking done 8000 - 8100\n",
      "df create done 8000 - 8100\n",
      "df to list done 8000 - 8100 \n",
      "\n",
      "coref done 8100 - 8200\n",
      "Could not extract relationships for text\n",
      "entity linking done 8100 - 8200\n",
      "df create done 8100 - 8200\n",
      "df to list done 8100 - 8200 \n",
      "\n",
      "coref done 8200 - 8300\n",
      "entity linking done 8200 - 8300\n",
      "df create done 8200 - 8300\n",
      "df to list done 8200 - 8300 \n",
      "\n",
      "coref done 8300 - 8400\n",
      "entity linking done 8300 - 8400\n",
      "df create done 8300 - 8400\n",
      "df to list done 8300 - 8400 \n",
      "\n",
      "coref done 8400 - 8500\n",
      "entity linking done 8400 - 8500\n",
      "df create done 8400 - 8500\n",
      "df to list done 8400 - 8500 \n",
      "\n",
      "coref done 8500 - 8600\n",
      "entity linking done 8500 - 8600\n",
      "df create done 8500 - 8600\n",
      "df to list done 8500 - 8600 \n",
      "\n",
      "coref done 8600 - 8700\n",
      "entity linking done 8600 - 8700\n",
      "df create done 8600 - 8700\n",
      "df to list done 8600 - 8700 \n",
      "\n",
      "coref done 8700 - 8800\n",
      "entity linking done 8700 - 8800\n",
      "df create done 8700 - 8800\n",
      "df to list done 8700 - 8800 \n",
      "\n",
      "coref done 8800 - 8900\n",
      "Could not extract relationships for text\n",
      "entity linking done 8800 - 8900\n",
      "df create done 8800 - 8900\n",
      "df to list done 8800 - 8900 \n",
      "\n",
      "coref done 8900 - 9000\n",
      "entity linking done 8900 - 9000\n",
      "df create done 8900 - 9000\n",
      "df to list done 8900 - 9000 \n",
      "\n",
      "coref done 9000 - 9100\n",
      "Could not extract relationships for text\n",
      "entity linking done 9000 - 9100\n",
      "df create done 9000 - 9100\n",
      "df to list done 9000 - 9100 \n",
      "\n",
      "coref done 9100 - 9200\n",
      "entity linking done 9100 - 9200\n",
      "df create done 9100 - 9200\n",
      "df to list done 9100 - 9200 \n",
      "\n",
      "coref done 9200 - 9300\n",
      "entity linking done 9200 - 9300\n",
      "df create done 9200 - 9300\n",
      "df to list done 9200 - 9300 \n",
      "\n",
      "coref done 9300 - 9400\n",
      "entity linking done 9300 - 9400\n",
      "df create done 9300 - 9400\n",
      "df to list done 9300 - 9400 \n",
      "\n",
      "coref done 9400 - 9500\n",
      "entity linking done 9400 - 9500\n",
      "df create done 9400 - 9500\n",
      "df to list done 9400 - 9500 \n",
      "\n",
      "coref done 9500 - 9600\n",
      "entity linking done 9500 - 9600\n",
      "df create done 9500 - 9600\n",
      "df to list done 9500 - 9600 \n",
      "\n",
      "coref done 9600 - 9700\n",
      "entity linking done 9600 - 9700\n",
      "df create done 9600 - 9700\n",
      "df to list done 9600 - 9700 \n",
      "\n",
      "coref done 9700 - 9800\n",
      "entity linking done 9700 - 9800\n",
      "df create done 9700 - 9800\n",
      "df to list done 9700 - 9800 \n",
      "\n",
      "coref done 9800 - 9900\n",
      "entity linking done 9800 - 9900\n",
      "df create done 9800 - 9900\n",
      "df to list done 9800 - 9900 \n",
      "\n",
      "coref done 9900 - 10000\n",
      "Could not extract relationships for text\n",
      "entity linking done 9900 - 10000\n",
      "df create done 9900 - 10000\n",
      "df to list done 9900 - 10000 \n",
      "\n",
      "coref done 10000 - 10100\n",
      "entity linking done 10000 - 10100\n",
      "df create done 10000 - 10100\n",
      "df to list done 10000 - 10100 \n",
      "\n",
      "coref done 10100 - 10200\n",
      "entity linking done 10100 - 10200\n",
      "df create done 10100 - 10200\n",
      "df to list done 10100 - 10200 \n",
      "\n",
      "coref done 10200 - 10300\n",
      "entity linking done 10200 - 10300\n",
      "df create done 10200 - 10300\n",
      "df to list done 10200 - 10300 \n",
      "\n",
      "coref done 10300 - 10400\n"
     ]
    }
   ],
   "source": [
    "# Calculate and save per paper\n",
    "win_size = 100\n",
    "start_point = 0\n",
    "entities_df_lst = []\n",
    "for j in data_df['file'].unique():\n",
    "    print('_______________________________\\n\\n', j)\n",
    "    df = data_df[data_df['file']==j]\n",
    "    for i in range(start_point, len(df), win_size):\n",
    "        coref_series = coref_res(text_series=df[\"text\"].iloc[i:i+win_size])\n",
    "        print('coref done', i, '-', i+win_size)\n",
    "        link_entities_series = link_entities(text_series=coref_series)\n",
    "        print('entity linking done', i, '-', i+win_size)\n",
    "        entities_df = pd.concat(link_entities_series.tolist())\n",
    "        print('df create done', i, '-', i+win_size)\n",
    "        entities_df_lst.append(entities_df)\n",
    "        print('df to list done', i, '-', i+win_size, '\\n')\n",
    "    all_entities_df = pd.concat(entities_df_lst)\n",
    "    all_entities_df.reset_index(drop=True, inplace=True)\n",
    "    edge_lst_df = all_entities_df.value_counts().reset_index().rename(columns={0: \"count\"})\n",
    "    edge_lst_df.to_csv('entity_weighted_edgelist_ALL_'+j[:-4]+'.csv')\n",
    "    edge_lst_df.to_feather('entity_weighted_edgelist_ALL_'+j[:-4]+'.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d36c245-8c59-4c34-8ec0-f4661faf0cf6",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-10-12T13:40:06.159766Z",
     "iopub.status.idle": "2022-10-12T13:40:06.160766Z",
     "shell.execute_reply": "2022-10-12T13:40:06.160266Z",
     "shell.execute_reply.started": "2022-10-12T13:40:06.160266Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# win_size = 1000\n",
    "# start_point = 0 #46000 # default to 0\n",
    "# entities_df_lst = []\n",
    "# for i in range(start_point, len(data_df), win_size):\n",
    "#     coref_series = coref_res(text_series=data_df[\"text\"].iloc[i:i+win_size])\n",
    "#     print('coref done', i, '-', i+win_size)\n",
    "#     link_entities_series = link_entities(text_series=coref_series)\n",
    "#     print('entity linking done', i, '-', i+win_size)\n",
    "#     entities_df = pd.concat(link_entities_series.tolist())\n",
    "#     print('df create done', i, '-', i+win_size)\n",
    "#     entities_df_lst.append(entities_df)\n",
    "#     print('df to list done', i, '-', i+win_size, '\\n')\n",
    "# all_entities_df = pd.concat(entities_df_lst)\n",
    "# # all_entities_df.reset_index(drop=True, inplace=True)\n",
    "# edge_lst_df = all_entities_df.value_counts().reset_index().rename(columns={0: \"count\"})\n",
    "# edge_lst_df.to_csv('entity_weighted_edgelist_ALL.csv')\n",
    "# edge_lst_df.to_feather('entity_weighted_edgelist_ALL.feather')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
